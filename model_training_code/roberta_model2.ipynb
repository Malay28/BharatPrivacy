{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malayshikhar/anaconda3/envs/TDL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 70/70 [16:54<00:00, 14.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training Loss: 0.3963406349931444, Validation Accuracy: 0.8495842781557067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [14:43<00:00, 12.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 0.2938976890274457, Validation Accuracy: 0.8790627362055934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [14:46<00:00, 12.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training Loss: 0.2129213680114065, Validation Accuracy: 0.8858654572940288\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.28      0.42       199\n",
      "     neutral       0.89      0.99      0.94      1124\n",
      "\n",
      "    accuracy                           0.89      1323\n",
      "   macro avg       0.89      0.64      0.68      1323\n",
      "weighted avg       0.89      0.89      0.86      1323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"TOSDR_labeled.csv\")\n",
    "\n",
    "label_map = {\"bad\": 0, \"neutral\": 1}\n",
    "df['Point'] = df['Point'].map(label_map)\n",
    "df = df.dropna()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['QouteText'].values,\n",
    "                                                                    df['Point'].values,\n",
    "                                                                    test_size=0.23,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_len = 128\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=max_len)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=max_len)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                              torch.tensor(train_encodings['attention_mask']),\n",
    "                              torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(torch.tensor(val_encodings['input_ids']),\n",
    "                            torch.tensor(val_encodings['attention_mask']),\n",
    "                            torch.tensor(val_labels))\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    bad_confidences = []  # Confidence scores for the \"bad\" class\n",
    "    for batch in val_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1]}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        bad_confidence = probs[:, 0].detach().cpu().numpy()  # Confidence scores for the \"bad\" class\n",
    "        bad_confidences.extend(bad_confidence)\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        val_preds.extend(preds)\n",
    "        val_labels.extend(batch[2].cpu().numpy())\n",
    "\n",
    "    val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss / len(train_loader)}, Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "val_preds = np.array(val_preds)\n",
    "val_labels = np.array(val_labels)\n",
    "print(classification_report(val_labels, val_preds, target_names=label_map.keys()))\n",
    "\n",
    "# Additional analysis using bad_confidences\n",
    "# You can use bad_confidences to analyze the intensity of badness for sentences labeled as \"bad\"\n",
    "# bad_indices = np.where(val_labels == 0)[0]  # Indices of sentences labeled as \"bad\"\n",
    "# for idx in bad_indices:\n",
    "#     print(f\"Sentence: {val_texts[idx]}, Badness Confidence: {bad_confidences[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badness Confidence: 0.7146732211112976\n",
      "Badness Confidence: 0.042185228317976\n",
      "Badness Confidence: 0.28201979398727417\n",
      "Badness Confidence: 0.5453076958656311\n",
      "Badness Confidence: 0.33847421407699585\n",
      "Badness Confidence: 0.3652246296405792\n",
      "Badness Confidence: 0.0923534408211708\n",
      "Badness Confidence: 0.5594250559806824\n",
      "Badness Confidence: 0.8760133981704712\n",
      "Badness Confidence: 0.30464163422584534\n"
     ]
    }
   ],
   "source": [
    "bad_indices = np.where(val_labels == 0)[0]  # Indices of sentences labeled as \"bad\"\n",
    "for idx in bad_indices[:10]:\n",
    "    print(f\"Badness Confidence: {bad_confidences[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for the sentence: bad\n",
      "Risk Level: 0.8488851189613342\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def predict_sentence(sentence, model, tokenizer, device, max_len=128):\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=max_len)\n",
    "    \n",
    "    # Move tensors to the appropriate device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Interpret the output to get the predicted label and confidence scores\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    confidence_score = probs[:, predicted_label].item()\n",
    "    \n",
    "    return predicted_label, confidence_score\n",
    "\n",
    "# Example sentence to predict\n",
    "sentence_to_predict = \"Please note that if you request the erasure of your personal information: We may retain some of your personal information as necessary for our legitimate business interests, such as fraud detection and prevention and enhancing safety.For example, if we suspend an Airbnb Account for fraud or safety reasons, we may retain certain information from that Airbnb Account to prevent that Member from opening a new Airbnb Account in the future.We may retain and use your personal information to the extent necessary to comply with our legal obligations.For example, Airbnb and Airbnb Payments may keep some of your information for tax, legal reporting and auditing obligations.\" \n",
    "\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_save_path = \"roberta_model\"\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Predict using the loaded model\n",
    "predicted_label, confidence_score = predict_sentence(sentence_to_predict, loaded_model, loaded_tokenizer, device)\n",
    "\n",
    "label_map = {\"bad\": 0, \"neutral\": 1}\n",
    "\n",
    "# Map the predicted label to the original class label\n",
    "label_map_reverse = {v: k for k, v in label_map.items()}\n",
    "predicted_class = label_map_reverse[predicted_label]\n",
    "\n",
    "print(\"Predicted class for the sentence:\", predicted_class)\n",
    "print(\"Risk Level:\", confidence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"roberta_model2\"\n",
    "\n",
    "model.save_pretrained(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
