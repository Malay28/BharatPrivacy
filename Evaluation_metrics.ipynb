{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malayshikhar/anaconda3/envs/TDL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, BartForConditionalGeneration, BartTokenizer,MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "\n",
    "bart_model_name = \"fine_tuned_bart_model_eng_to_engSummary\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def generate_summary(text, max_length=100):\n",
    "    inputs = bart_tokenizer(text, max_length=max_length, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "    summary_ids = bart_model.generate(input_ids, max_length=150, min_length=10, length_penalty=2.0, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "df = pd.read_csv(\"TOSDR_labeled_with_summaries.csv\")\n",
    "\n",
    "def generate_summary_for_row(row):\n",
    "    text = row[\"Text\"]\n",
    "    generated_summary = generate_summary(text)\n",
    "    return generated_summary\n",
    "\n",
    "\n",
    "df[\"gen_summary\"] = df.apply(generate_summary_for_row, axis=1)\n",
    "\n",
    "\n",
    "df.to_csv(\"TOSDR_labeled_with_summaries_and_gen.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score: 0.405466266173145\n",
      "ROUGE-2 Score: 0.19073618897552436\n",
      "ROUGE-L Score: 0.33643418896114047\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the dataset from CSV\n",
    "dataset_path = \"TOSDR_labeled_with_summaries_and_gen.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract reference and generated summaries from the dataset\n",
    "eng_summaries = data[\"eng_summary\"].tolist()\n",
    "gen_summaries = data[\"gen_summary\"].tolist()\n",
    "\n",
    "# Load BART model and tokenizer\n",
    "bart_model_name = \"fine_tuned_bart_model_eng_to_engSummary\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "\n",
    "# Tokenize the summaries\n",
    "def tokenize_summaries(summaries, tokenizer):\n",
    "    tokenized_summaries = tokenizer(summaries, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokenized_summaries\n",
    "\n",
    "eng_tokenized = tokenize_summaries(eng_summaries, bart_tokenizer)\n",
    "gen_tokenized = tokenize_summaries(gen_summaries, bart_tokenizer)\n",
    "\n",
    "\n",
    "def calculate_rouge(references, candidates):\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        scores = scorer.score(ref, cand)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    return rouge1_scores, rouge2_scores, rougeL_scores\n",
    "\n",
    "rouge1, rouge2, rougeL = calculate_rouge(eng_summaries, gen_summaries)\n",
    "\n",
    "\n",
    "print(\"ROUGE-1 Score:\", sum(rouge1)/len(rouge1))\n",
    "print(\"ROUGE-2 Score:\", sum(rouge2)/len(rouge2))\n",
    "print(\"ROUGE-L Score:\", sum(rougeL)/len(rougeL))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
