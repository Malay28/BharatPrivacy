{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/malayshikhar/anaconda3/envs/TDL/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 188/188 [48:19<00:00, 15.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 188/188 [45:56<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 2.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 188/188 [53:28<00:00, 17.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 2.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:46<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 2.000438553946359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_output_length=150):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        output_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_output_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.flatten(),\n",
    "            \"attention_mask\": input_encoding.attention_mask.flatten(),\n",
    "            \"labels\": output_encoding.input_ids.flatten(),\n",
    "            \"decoder_attention_mask\": output_encoding.attention_mask.flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "dataset_path = \"TOSDR_labeled_with_summaries.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "texts = df['Text'].tolist()\n",
    "summaries = df['eng_summary'].tolist()\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(texts, summaries, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_summaries, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_summaries, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Epoch \" + str(epoch + 1)):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Epoch {} Loss: {:.4f}\".format(epoch + 1, total_loss / len(train_dataloader)))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    decoder_attention_mask = batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "print(\"Average Validation Loss:\", avg_val_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: cookies and other tracking technologies are used to improve your browsing experience, personalize content, and serve\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model_path = \"path_to_your_saved_model\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "long_english_sentence = \"We use cookies and other tracking technologies to improve your browsing experience, personalize content, and serve targeted ads.\"\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + long_english_sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=20, num_beams=2, early_stopping=True)\n",
    "\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Summary: you agree to indemnify and hold Apple, its officers, directors, shareholders, predecessors\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "# model_path = \"path_to_your_saved_model\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "english_text = \"You agree to indemnify and hold Apple, its officers, directors, shareholders, predecessors, successors in interest, employees, agents, subsidiaries and affiliates, harmless from any demands, loss, liability, claims or expenses (including attorneys’ fees), made against Apple by any third party due to or arising out of or in connection with your use of the Site.\"\n",
    "\n",
    "inputs = tokenizer.encode(\"summarize: \" + english_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=20, num_beams=2, early_stopping=True, length_penalty=2.0)\n",
    "\n",
    "hindi_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Hindi Summary:\", hindi_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
